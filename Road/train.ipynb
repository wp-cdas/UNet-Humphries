{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from data import *\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "trainGene = trainGenerator(4,\n",
    "                           '/data/spacenet/road/AllTrain',\n",
    "                           'PAN-8BIT',\n",
    "                           'PAN-GT',\n",
    "                           data_gen_args,\n",
    "                           save_to_dir = None)\n",
    "validGene = trainGenerator(4,\n",
    "                           '/data/spacenet/road/AllValid',\n",
    "                           'PAN-8BIT',\n",
    "                           'PAN-GT',\n",
    "                           data_gen_args,\n",
    "                           save_to_dir = None)\n",
    "model = unet()\n",
    "model_checkpoint = ModelCheckpoint('/lfs/jonas/oldunet/roadweights.hdf5', \n",
    "                                   monitor='loss',\n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('/lfs/jonas/oldunet/weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1540 images belonging to 1 classes.\n",
      "Found 1540 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      " 2/20 [==>...........................] - ETA: 1s - loss: 1.2398 - accuracy: 0.0648WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0479s vs `on_train_batch_end` time: 0.0803s). Check your callbacks.\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.7478 - accuracy: 0.8483Found 1240 images belonging to 1 classes.\n",
      "Found 1240 images belonging to 1 classes.\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.74775, saving model to /lfs/jonas/oldunet/roadweights.hdf5\n",
      "20/20 [==============================] - 8s 390ms/step - loss: 0.7478 - accuracy: 0.8483 - val_loss: 0.6923 - val_accuracy: 0.9620\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.9560\n",
      "Epoch 00002: loss improved from 0.74775 to 0.69186, saving model to /lfs/jonas/oldunet/roadweights.hdf5\n",
      "20/20 [==============================] - 5s 244ms/step - loss: 0.6919 - accuracy: 0.9560 - val_loss: 0.6914 - val_accuracy: 0.9492\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.9566\n",
      "Epoch 00003: loss improved from 0.69186 to 0.69096, saving model to /lfs/jonas/oldunet/roadweights.hdf5\n",
      "20/20 [==============================] - 5s 239ms/step - loss: 0.6910 - accuracy: 0.9566 - val_loss: 0.6905 - val_accuracy: 0.9531\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6901 - accuracy: 0.9549\n",
      "Epoch 00004: loss improved from 0.69096 to 0.69008, saving model to /lfs/jonas/oldunet/roadweights.hdf5\n",
      "20/20 [==============================] - 5s 255ms/step - loss: 0.6901 - accuracy: 0.9549 - val_loss: 0.6894 - val_accuracy: 0.9763\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.9561\n",
      "Epoch 00005: loss improved from 0.69008 to 0.68918, saving model to /lfs/jonas/oldunet/roadweights.hdf5\n",
      "20/20 [==============================] - 5s 244ms/step - loss: 0.6892 - accuracy: 0.9561 - val_loss: 0.6886 - val_accuracy: 0.9653\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6882 - accuracy: 0.9589\n",
      "Epoch 00006: loss improved from 0.68918 to 0.68824, saving model to /lfs/jonas/oldunet/roadweights.hdf5\n",
      "20/20 [==============================] - 5s 237ms/step - loss: 0.6882 - accuracy: 0.9589 - val_loss: 0.6876 - val_accuracy: 0.9701\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.9576\n",
      "Epoch 00007: loss improved from 0.68824 to 0.68736, saving model to /lfs/jonas/oldunet/roadweights.hdf5\n",
      "20/20 [==============================] - 5s 236ms/step - loss: 0.6874 - accuracy: 0.9576 - val_loss: 0.6869 - val_accuracy: 0.9529\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6864 - accuracy: 0.9594\n",
      "Epoch 00008: loss improved from 0.68736 to 0.68643, saving model to /lfs/jonas/oldunet/roadweights.hdf5\n",
      "20/20 [==============================] - 5s 242ms/step - loss: 0.6864 - accuracy: 0.9594 - val_loss: 0.6861 - val_accuracy: 0.9519\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6856 - accuracy: 0.9564\n",
      "Epoch 00009: loss improved from 0.68643 to 0.68557, saving model to /lfs/jonas/oldunet/roadweights.hdf5\n",
      "20/20 [==============================] - 5s 238ms/step - loss: 0.6856 - accuracy: 0.9564 - val_loss: 0.6851 - val_accuracy: 0.9552\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6847 - accuracy: 0.9544\n",
      "Epoch 00010: loss improved from 0.68557 to 0.68471, saving model to /lfs/jonas/oldunet/roadweights.hdf5\n",
      "20/20 [==============================] - 5s 231ms/step - loss: 0.6847 - accuracy: 0.9544 - val_loss: 0.6844 - val_accuracy: 0.9479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9cb24d3880>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainGene,\n",
    "          steps_per_epoch=20,\n",
    "          epochs=10,\n",
    "          callbacks=[model_checkpoint], \n",
    "          validation_data=validGene, \n",
    "          validation_steps=2,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from skimage.io import imread\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage.transform as trans\n",
    "test_path = '/data/spacenet/bldg/AllTest/PAN-PNG/'\n",
    "test_images = glob(test_path + '*.png')\n",
    "image_size = (256, 256, 1)\n",
    "sample_size = 4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_sample = random.sample(test_images, sample_size)\n",
    "samp_imgs = []\n",
    "for sample in test_sample:\n",
    "    samp_img = imread(sample)\n",
    "    #ranCor = np.int16(random.uniform(0, samp_img.shape[0]-image_size[0]))\n",
    "    #samp_img = samp_img[ranCor:ranCor+image_size[0], ranCor:ranCor+image_size[1]].reshape(image_size)\n",
    "    samp_img = trans.resize(samp_img, image_size)\n",
    "    samp_imgs += [samp_img]\n",
    "samp_imgs = np.array(samp_imgs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preds = model.predict(samp_imgs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, sample_size, figsize=(50, 50))\n",
    "for idx, ax in enumerate(axs[0]):\n",
    "    ax.imshow(samp_imgs[idx, :, :, 0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "for idx, ax in enumerate(axs[1]):\n",
    "    ax.imshow(preds[idx, :, :, 0], cmap = 'gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "threshes = []\n",
    "for pred in preds:\n",
    "    pred = np.int16(pred * 255)[:,:,0]\n",
    "    gray = cv2.cvtColor(pred, cv2.COLOR_BGR2GRAY)\n",
    "    #blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    #thresh = cv2.threshold(gray, 0.25, 1, cv2.THRESH_BINARY)[1]\n",
    "    ret2,thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    plt.imshow(thresh)\n",
    "    threshes += [thresh]\n",
    "threshes = np.array(threshes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#threshes.shape\n",
    "plt.imshow(threshes[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
